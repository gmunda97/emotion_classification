{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Recurrent Neural Network for text classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/lstm-text-classification-using-pytorch-2c6c657f8fc0 \n",
    "\n",
    "https://keras.io/examples/nlp/text_classification_with_transformer/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../Datasets/data_train.csv\")\n",
    "val_data = pd.read_csv(\"../Datasets/data_val.csv\")\n",
    "test_data = pd.read_csv(\"../Datasets/data_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 64\n",
    "OUTPUT_DIM = 8\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokenizer\n",
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "\n",
    "# tokenize sentences\n",
    "train_data['tokens'] = train_data['Text'].apply(tokenizer)\n",
    "val_data['tokens'] = val_data['Text'].apply(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the frequency of each word in the training data\n",
    "word_freq = Counter()\n",
    "for sentence in train_data['Text']:\n",
    "    word_freq.update(tokenizer(sentence))\n",
    "\n",
    "# build vocabulary\n",
    "vocab = Vocab(word_freq)\n",
    "\n",
    "# convert tokens to indices\n",
    "train_data['text_tensor'] = train_data['Text'].apply(lambda x: torch.LongTensor([vocab[token] for token in tokenizer(x)]))\n",
    "train_data['label_tensor'] = train_data['Emotion'].apply(lambda x: torch.LongTensor([vocab[token] for token in tokenizer(x)]))\n",
    "val_data['text_tensor'] = val_data['Text'].apply(lambda x: torch.LongTensor([vocab[token] for token in tokenizer(x)]))\n",
    "val_data['label_tensor'] = val_data['Emotion'].apply(lambda x: torch.LongTensor([vocab[token] for token in tokenizer(x)]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [classes for classes in train_data['Emotion'].unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = collections.Counter()\n",
    "for line in train_data[\"Text\"].to_list():\n",
    "    counter.update(tokenizer(line))\n",
    "vocab = torchtext.vocab.vocab(counter, min_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sentence = train_data[\"Text\"][0]\n",
    "second_sentence = train_data[\"Text\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size if 15212\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 5, 8, 9, 10, 3, 11, 12, 13, 14, 15, 16, 17, 18]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "print(f\"Vocab size if {vocab_size}\")\n",
    "\n",
    "def encode(x):\n",
    "    return [vocab.get_stoi()[s] for s in tokenizer(x)]\n",
    "\n",
    "vec = encode(first_sentence)\n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'can',\n",
       " 'go',\n",
       " 'from',\n",
       " 'feeling',\n",
       " 'so',\n",
       " 'hopeless',\n",
       " 'to',\n",
       " 'so',\n",
       " 'damned',\n",
       " 'hopeful',\n",
       " 'just',\n",
       " 'from',\n",
       " 'being',\n",
       " 'around',\n",
       " 'someone',\n",
       " 'who',\n",
       " 'cares',\n",
       " 'and',\n",
       " 'is',\n",
       " 'awake']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode(x):\n",
    "    return [vocab.get_itos()[i] for i in x]\n",
    "\n",
    "decode(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padify(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label, \n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [encode(x[1]) for x in b]\n",
    "    # first, compute max length of a sequence in this minibatch\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0]-1 for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Sentence in dataset:\n",
      "i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake\n",
      "Length: 108\n",
      "\n",
      "Second Sentence in dataset:\n",
      "im grabbing a minute to post i feel greedy wrong\n",
      "Length:  48\n"
     ]
    }
   ],
   "source": [
    "f_tokens = encode(first_sentence)\n",
    "s_tokens = encode(second_sentence)\n",
    "\n",
    "print(f'First Sentence in dataset:\\n{first_sentence}')\n",
    "print(\"Length:\", len(train_data[\"Text\"][0]))\n",
    "print(f'\\nSecond Sentence in dataset:\\n{second_sentence}')\n",
    "print(\"Length: \", len(train_data[\"Text\"][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample text:\n",
      "i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake\n",
      "\n",
      "BoW vector:\n",
      "tensor([1., 0., 0.,  ..., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "def to_bow(text,bow_vocab_size=vocab_size):\n",
    "    res = torch.zeros(bow_vocab_size,dtype=torch.float32)\n",
    "    for i in encode(text):\n",
    "        if i<bow_vocab_size:\n",
    "            res[i] += 1\n",
    "    return res\n",
    "\n",
    "print(f\"sample text:\\n{train_data['Text'][0]}\")\n",
    "print(f\"\\nBoW vector:\\n{to_bow(train_data['Text'][1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this collate function gets list of batch_size tuples, and needs to \n",
    "# return a pair of label-feature tensors for the whole minibatch\n",
    "def bowify(b):\n",
    "    return (\n",
    "            torch.LongTensor([t[0]-1 for t in b]),\n",
    "            torch.stack([to_bow(t[1]) for t in b])\n",
    "    )\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=16, collate_fn=bowify, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=16, collate_fn=bowify, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Sequential(torch.nn.Linear(vocab_size,4),torch.nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(net, dataloader, lr=0.01, optimizer=None, loss_fn = torch.nn.NLLLoss(),epoch_size=None, report_freq=200):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(),lr=lr)\n",
    "    net.train()\n",
    "    total_loss,acc,count,i = 0,0,0,0\n",
    "    for labels,features in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        out = net(features)\n",
    "        loss = loss_fn(out,labels) #cross_entropy(out,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss\n",
    "        _,predicted = torch.max(out,1)\n",
    "        acc+=(predicted==labels).sum()\n",
    "        count+=len(labels)\n",
    "        i+=1\n",
    "        if i%report_freq==0:\n",
    "            print(f\"{count}: acc={acc.item()/count}\")\n",
    "        if epoch_size and count>epoch_size:\n",
    "            break\n",
    "    return total_loss.item()/count, acc.item()/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "6582",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/MSc Multilingual Technologies/2nd Semester/Machine Learning Methods for Language Processing/Final_project/emotion_classification/emotion-venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/Documents/MSc Multilingual Technologies/2nd Semester/Machine Learning Methods for Language Processing/Final_project/emotion_classification/emotion-venv/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/MSc Multilingual Technologies/2nd Semester/Machine Learning Methods for Language Processing/Final_project/emotion_classification/emotion-venv/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 6582",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_epoch(net,train_loader,epoch_size\u001b[39m=\u001b[39;49m\u001b[39m1600\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[55], line 5\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(net, dataloader, lr, optimizer, loss_fn, epoch_size, report_freq)\u001b[0m\n\u001b[1;32m      3\u001b[0m net\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      4\u001b[0m total_loss,acc,count,i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfor\u001b[39;00m labels,features \u001b[39min\u001b[39;00m dataloader:\n\u001b[1;32m      6\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m      7\u001b[0m     out \u001b[39m=\u001b[39m net(features)\n",
      "File \u001b[0;32m~/Documents/MSc Multilingual Technologies/2nd Semester/Machine Learning Methods for Language Processing/Final_project/emotion_classification/emotion-venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Documents/MSc Multilingual Technologies/2nd Semester/Machine Learning Methods for Language Processing/Final_project/emotion_classification/emotion-venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Documents/MSc Multilingual Technologies/2nd Semester/Machine Learning Methods for Language Processing/Final_project/emotion_classification/emotion-venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Documents/MSc Multilingual Technologies/2nd Semester/Machine Learning Methods for Language Processing/Final_project/emotion_classification/emotion-venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Documents/MSc Multilingual Technologies/2nd Semester/Machine Learning Methods for Language Processing/Final_project/emotion_classification/emotion-venv/lib/python3.10/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3808\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Documents/MSc Multilingual Technologies/2nd Semester/Machine Learning Methods for Language Processing/Final_project/emotion_classification/emotion-venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 6582"
     ]
    }
   ],
   "source": [
    "train_epoch(net,train_loader,epoch_size=1600)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Part works now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/giacomomunda/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/giacomomunda/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/giacomomunda/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, df, vectorizer, label_encoder):\n",
    "        self.df = df\n",
    "        self.vectorizer = vectorizer\n",
    "        self.label_encoder = label_encoder\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.punctuation = set(string.punctuation)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [token.lower() for token in tokens if token not in self.stop_words and token not in self.punctuation]\n",
    "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "        preprocess_text = \" \".join(tokens)\n",
    "        return preprocess_text\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.iloc[idx][\"Text\"]\n",
    "        label = self.df.iloc[idx][\"Emotion\"]\n",
    "\n",
    "        # Preprocess text\n",
    "        text = self.preprocess_text(text)\n",
    "        \n",
    "        # Convert text to BOW vector\n",
    "        bow = self.vectorizer.transform([text]).toarray()[0]\n",
    "        \n",
    "        # Convert label to numerical value\n",
    "        label = self.label_encoder.transform([label])[0]\n",
    "        \n",
    "        return torch.LongTensor([label]), torch.FloatTensor(bow)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.log_softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from pandas dataframe\n",
    "train_df = pd.read_csv(\"../Datasets/data_train.csv\")\n",
    "eval_df = pd.read_csv(\"../Datasets/data_val.csv\")\n",
    "test_df = pd.read_csv(\"../Datasets/data_test.csv\")\n",
    "\n",
    "# Create sorted list of unique labels\n",
    "all_labels = np.concatenate([train_df[\"Emotion\"].unique(), eval_df[\"Emotion\"].unique()])\n",
    "unique_labels = np.unique(all_labels)\n",
    "sorted_labels = np.sort(unique_labels)\n",
    "\n",
    "# Initialize vectorizer and label encoder\n",
    "vectorizer = CountVectorizer()\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit vectorizer and label encoder to training data\n",
    "vectorizer.fit(train_df[\"Text\"])\n",
    "label_encoder.fit(train_df[\"Emotion\"])\n",
    "\n",
    "# Create dataset and data loader for training data\n",
    "train_dataset = TextClassificationDataset(train_df, vectorizer, label_encoder)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Fit a new label encoder to the evaluation labels only\n",
    "eval_labels = eval_df[\"Emotion\"]\n",
    "eval_label_encoder = LabelEncoder()\n",
    "eval_label_encoder.fit(eval_labels)\n",
    "\n",
    "# Create dataset and data loader for evaluation data\n",
    "eval_dataset = TextClassificationDataset(eval_df, vectorizer, eval_label_encoder)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Fit a new label encoder to the test labels only\n",
    "test_labels = test_df[\"Emotion\"]\n",
    "test_label_encoder = LabelEncoder()\n",
    "test_label_encoder.fit(test_labels)\n",
    "\n",
    "# Create dataset and data loader for test data\n",
    "test_dataset = TextClassificationDataset(test_df, vectorizer, test_label_encoder)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: training loss = 0.0688542762661928\n",
      "Accuracy: 0.8494247123561781\n",
      "Test accuracy: 0.8569284642321161\n",
      "Epoch 1: training loss = 0.017362684891148502\n",
      "Accuracy: 0.8934467233616809\n",
      "Test accuracy: 0.8899449724862432\n",
      "Epoch 2: training loss = 0.007776901100818828\n",
      "Accuracy: 0.896448224112056\n",
      "Test accuracy: 0.887943971985993\n",
      "Epoch 3: training loss = 0.004871668008487518\n",
      "Accuracy: 0.8959479739869936\n",
      "Test accuracy: 0.8854427213606804\n",
      "Epoch 4: training loss = 0.0034349635364349807\n",
      "Accuracy: 0.8984492246123061\n",
      "Test accuracy: 0.8844422211105553\n",
      "Epoch 5: training loss = 0.0025763971321855826\n",
      "Accuracy: 0.8959479739869936\n",
      "Test accuracy: 0.8874437218609305\n",
      "Epoch 6: training loss = 0.001983615933085245\n",
      "Accuracy: 0.886943471735868\n",
      "Test accuracy: 0.8804402201100551\n",
      "Epoch 7: training loss = 0.0016071952151689394\n",
      "Accuracy: 0.8929464732366184\n",
      "Test accuracy: 0.8814407203601801\n"
     ]
    }
   ],
   "source": [
    "net = NeuralNetwork(len(vectorizer.vocabulary_), 20, 6)\n",
    "\n",
    "# define loss function and optimizer\n",
    "loss_fn = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "best_val_accuracy = 0\n",
    "best_model = None\n",
    "\n",
    "# Train model\n",
    "for epoch in range(8):\n",
    "    total_loss, total_acc, count = 0, 0, 0\n",
    "    net.train()\n",
    "    for labels, features in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        out = net(features)\n",
    "        labels = labels.squeeze(1)\n",
    "        loss = loss_fn(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch}: training loss = {total_loss/len(train_dataset)}\")\n",
    "\n",
    "    # Evaluate model on validation set\n",
    "    net.eval()\n",
    "    total_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for labels, features in eval_dataloader:\n",
    "            out = net(features)\n",
    "            labels = labels.squeeze(1)\n",
    "            _, predicted = torch.max(out, dim=1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "        val_accuracy = total_correct / len(eval_dataset)\n",
    "        \n",
    "    print(f\"Accuracy: {val_accuracy}\")\n",
    "\n",
    "    # Save the best model based on validation accuracy\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_model = net.state_dict()\n",
    "        torch.save(best_model, \"best_model.pt\")\n",
    "\n",
    "    # Evaluate model on test set\n",
    "    net.load_state_dict(best_model)\n",
    "    net.eval()\n",
    "    total_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for labels, features in test_dataloader:\n",
    "            out = net(features)\n",
    "            labels = labels.squeeze(1)\n",
    "            _, predicted = torch.max(out, dim=1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "        test_accuracy = total_correct / len(test_dataset)\n",
    "\n",
    "    print(f\"Test accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results:**\n",
    "\n",
    "- *Fully Connected with CountVectorizer:*\n",
    "\n",
    "    8 Epochs --> Accuracy: 0.8799 | Loss: 0.0017039 (best loss)\n",
    "\n",
    "    Best accuracy score: 0.8909 (Epoch 3)\n",
    "\n",
    "    After preprocessing --> best validation accuracy score:\n",
    "\n",
    "                            Epoch 5: 0.8984\n",
    "\n",
    "\n",
    "- *Fully Connected with TfidfVectorizer:*\n",
    "\n",
    "    8 epochs --> Accuracy: 0.8799 | Loss: 0.0026595\n",
    "\n",
    "    Best accuracy score: 0.8849\n",
    "\n",
    "    After preprocessing --> best accuracy score: 0.8879 (Epoch 6)\n",
    "                            \n",
    "                            Epoch 8: 0.8854"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Doing inference with the trained model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Today I went to the supermarket and bought some fruits, they were delicious!' | Predicted emotion: joy\n",
      "\n",
      "'I hate this movie' | Predicted emotion: joy\n",
      "\n",
      "'I was very pleased to see my daughter today' | Predicted emotion: joy\n",
      "\n",
      "'Why are you so angry?' | Predicted emotion: anger\n",
      "\n",
      "'Studying computational linguistics can be hard, but very satisfying!' | Predicted emotion: sadness\n",
      "\n",
      "'What are you doing for Christmas? I hope you have a great time' | Predicted emotion: sadness\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loaded_state_dict = torch.load(\"best_model.pt\")\n",
    "net.load_state_dict(loaded_state_dict)\n",
    "\n",
    "# prepare input data for inference\n",
    "new_data = [\"Today I went to the supermarket and bought some fruits, they were delicious!\", \n",
    "            \"I hate this movie\", \n",
    "            \"I was very pleased to see my daughter today\",\n",
    "            \"Why are you so angry?\",\n",
    "            \"Studying computational linguistics can be hard, but very satisfying!\",\n",
    "            \"What are you doing for Christmas? I hope you have a great time\"]\n",
    "\n",
    "# convert input data to BOW vectors\n",
    "new_data_bow = vectorizer.transform(new_data).toarray()\n",
    "\n",
    "# run model on input data\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    output_tensor = net(torch.FloatTensor(new_data_bow))\n",
    "    _, predicted = torch.max(output_tensor, dim=1)\n",
    "    predicted_labels = label_encoder.inverse_transform(predicted)\n",
    "    for i, sentence in enumerate(new_data):\n",
    "        print(f\"'{sentence}' | Predicted emotion: {predicted_labels[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotion-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
