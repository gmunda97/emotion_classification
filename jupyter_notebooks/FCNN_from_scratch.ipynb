{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Fully Connected Neural Network for text classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/lstm-text-classification-using-pytorch-2c6c657f8fc0 \n",
    "\n",
    "https://keras.io/examples/nlp/text_classification_with_transformer/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../Datasets/data_train.csv\")\n",
    "val_data = pd.read_csv(\"../Datasets/data_val.csv\")\n",
    "test_data = pd.read_csv(\"../Datasets/data_test.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/giacomomunda/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/giacomomunda/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/giacomomunda/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, df, vectorizer, label_encoder):\n",
    "        self.df = df\n",
    "        self.vectorizer = vectorizer\n",
    "        self.label_encoder = label_encoder\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.punctuation = set(string.punctuation)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [token.lower() for token in tokens if token not in self.stop_words and token not in self.punctuation]\n",
    "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "        preprocess_text = \" \".join(tokens)\n",
    "        return preprocess_text\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.iloc[idx][\"Text\"]\n",
    "        label = self.df.iloc[idx][\"Emotion\"]\n",
    "\n",
    "        # Preprocess text\n",
    "        text = self.preprocess_text(text)\n",
    "        \n",
    "        # Convert text to BOW vector\n",
    "        bow = self.vectorizer.transform([text]).toarray()[0]\n",
    "        \n",
    "        # Convert label to numerical value\n",
    "        label = self.label_encoder.transform([label])[0]\n",
    "        \n",
    "        return torch.LongTensor([label]), torch.FloatTensor(bow)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.log_softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from pandas dataframe\n",
    "train_df = pd.read_csv(\"../Datasets/data_train.csv\")\n",
    "eval_df = pd.read_csv(\"../Datasets/data_val.csv\")\n",
    "test_df = pd.read_csv(\"../Datasets/data_test.csv\")\n",
    "\n",
    "# Create sorted list of unique labels\n",
    "all_labels = np.concatenate([train_df[\"Emotion\"].unique(), eval_df[\"Emotion\"].unique()])\n",
    "unique_labels = np.unique(all_labels)\n",
    "sorted_labels = np.sort(unique_labels)\n",
    "\n",
    "# Initialize vectorizer and label encoder\n",
    "vectorizer = CountVectorizer()\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit vectorizer and label encoder to training data\n",
    "vectorizer.fit(train_df[\"Text\"])\n",
    "label_encoder.fit(train_df[\"Emotion\"])\n",
    "\n",
    "# Create dataset and data loader for training data\n",
    "train_dataset = TextClassificationDataset(train_df, vectorizer, label_encoder)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Fit a new label encoder to the evaluation labels only\n",
    "eval_labels = eval_df[\"Emotion\"]\n",
    "eval_label_encoder = LabelEncoder()\n",
    "eval_label_encoder.fit(eval_labels)\n",
    "\n",
    "# Create dataset and data loader for evaluation data\n",
    "eval_dataset = TextClassificationDataset(eval_df, vectorizer, eval_label_encoder)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Fit a new label encoder to the test labels only\n",
    "test_labels = test_df[\"Emotion\"]\n",
    "test_label_encoder = LabelEncoder()\n",
    "test_label_encoder.fit(test_labels)\n",
    "\n",
    "# Create dataset and data loader for test data\n",
    "test_dataset = TextClassificationDataset(test_df, vectorizer, test_label_encoder)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: training loss = 0.0688542762661928\n",
      "Accuracy: 0.8494247123561781\n",
      "Test accuracy: 0.8569284642321161\n",
      "Epoch 1: training loss = 0.017362684891148502\n",
      "Accuracy: 0.8934467233616809\n",
      "Test accuracy: 0.8899449724862432\n",
      "Epoch 2: training loss = 0.007776901100818828\n",
      "Accuracy: 0.896448224112056\n",
      "Test accuracy: 0.887943971985993\n",
      "Epoch 3: training loss = 0.004871668008487518\n",
      "Accuracy: 0.8959479739869936\n",
      "Test accuracy: 0.8854427213606804\n",
      "Epoch 4: training loss = 0.0034349635364349807\n",
      "Accuracy: 0.8984492246123061\n",
      "Test accuracy: 0.8844422211105553\n",
      "Epoch 5: training loss = 0.0025763971321855826\n",
      "Accuracy: 0.8959479739869936\n",
      "Test accuracy: 0.8874437218609305\n",
      "Epoch 6: training loss = 0.001983615933085245\n",
      "Accuracy: 0.886943471735868\n",
      "Test accuracy: 0.8804402201100551\n",
      "Epoch 7: training loss = 0.0016071952151689394\n",
      "Accuracy: 0.8929464732366184\n",
      "Test accuracy: 0.8814407203601801\n"
     ]
    }
   ],
   "source": [
    "net = NeuralNetwork(len(vectorizer.vocabulary_), 20, 6)\n",
    "\n",
    "# define loss function and optimizer\n",
    "loss_fn = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "best_val_accuracy = 0\n",
    "best_model = None\n",
    "\n",
    "# Train model\n",
    "for epoch in range(8):\n",
    "    total_loss, total_acc, count = 0, 0, 0\n",
    "    net.train()\n",
    "    for labels, features in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        out = net(features)\n",
    "        labels = labels.squeeze(1)\n",
    "        loss = loss_fn(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch}: training loss = {total_loss/len(train_dataset)}\")\n",
    "\n",
    "    # Evaluate model on validation set\n",
    "    net.eval()\n",
    "    total_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for labels, features in eval_dataloader:\n",
    "            out = net(features)\n",
    "            labels = labels.squeeze(1)\n",
    "            _, predicted = torch.max(out, dim=1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "        val_accuracy = total_correct / len(eval_dataset)\n",
    "        \n",
    "    print(f\"Accuracy: {val_accuracy}\")\n",
    "\n",
    "    # Save the best model based on validation accuracy\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_model = net.state_dict()\n",
    "        torch.save(best_model, \"best_model.pt\")\n",
    "\n",
    "    # Evaluate model on test set\n",
    "    net.load_state_dict(best_model)\n",
    "    net.eval()\n",
    "    total_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for labels, features in test_dataloader:\n",
    "            out = net(features)\n",
    "            labels = labels.squeeze(1)\n",
    "            _, predicted = torch.max(out, dim=1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "        test_accuracy = total_correct / len(test_dataset)\n",
    "\n",
    "    print(f\"Test accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results:**\n",
    "\n",
    "- *Fully Connected with CountVectorizer:*\n",
    "\n",
    "    8 Epochs --> Accuracy: 0.8799 | Loss: 0.0017039 (best loss)\n",
    "\n",
    "    Best accuracy score: 0.8909 (Epoch 3)\n",
    "\n",
    "    After preprocessing --> best validation accuracy score:\n",
    "\n",
    "                            Epoch 5: 0.8984\n",
    "\n",
    "\n",
    "- *Fully Connected with TfidfVectorizer:*\n",
    "\n",
    "    8 epochs --> Accuracy: 0.8799 | Loss: 0.0026595\n",
    "\n",
    "    Best accuracy score: 0.8849\n",
    "\n",
    "    After preprocessing --> best accuracy score: 0.8879 (Epoch 6)\n",
    "                            \n",
    "                            Epoch 8: 0.8854"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Doing inference with the trained model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Today I went to the supermarket and bought some fruits, they were delicious!' | Predicted emotion: joy\n",
      "\n",
      "'I hate this movie' | Predicted emotion: joy\n",
      "\n",
      "'I was very pleased to see my daughter today' | Predicted emotion: joy\n",
      "\n",
      "'Why are you so angry?' | Predicted emotion: anger\n",
      "\n",
      "'Studying computational linguistics can be hard, but very satisfying!' | Predicted emotion: sadness\n",
      "\n",
      "'What are you doing for Christmas? I hope you have a great time' | Predicted emotion: sadness\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loaded_state_dict = torch.load(\"best_model.pt\")\n",
    "net.load_state_dict(loaded_state_dict)\n",
    "\n",
    "# prepare input data for inference\n",
    "new_data = [\"Today I went to the supermarket and bought some fruits, they were delicious!\", \n",
    "            \"I hate this movie\", \n",
    "            \"I was very pleased to see my daughter today\",\n",
    "            \"Why are you so angry?\",\n",
    "            \"Studying computational linguistics can be hard, but very satisfying!\",\n",
    "            \"What are you doing for Christmas? I hope you have a great time\"]\n",
    "\n",
    "# convert input data to BOW vectors\n",
    "new_data_bow = vectorizer.transform(new_data).toarray()\n",
    "\n",
    "# run model on input data\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    output_tensor = net(torch.FloatTensor(new_data_bow))\n",
    "    _, predicted = torch.max(output_tensor, dim=1)\n",
    "    predicted_labels = label_encoder.inverse_transform(predicted)\n",
    "    for i, sentence in enumerate(new_data):\n",
    "        print(f\"'{sentence}' | Predicted emotion: {predicted_labels[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotion-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
